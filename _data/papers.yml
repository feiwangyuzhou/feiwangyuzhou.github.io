  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2017
    selected: no #yes/no
    title: >
      A Neural Local Coherence Model
    authors: Dat Nguyen, and Shafiq Joty
    id: PAMACLACL2017_Nguyen
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: (to appear)
    booktitle: >
      Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)
    doc-url: http://www.aclweb.org/anthology/P16-1165
    abstract: >
      We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.
    bibtex: >
      @inproceedings{dat-joty:2017,
       abstract = {We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.},
       address = {Vancouver, Canada},
       author = {Nguyen, Dat and Joty, Shafiq},
       booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)},
       link = {http://www.aclweb.org/anthology/P16-1165},
       month = {August},
       pages = {(to appear)},
       publisher = {Association for Computational Linguistics},
       series = {ACL '17},
       title = {A Neural Local Coherence Model},
       year = {2017}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2005
    selected: no #yes/no
    title: >
      Towards Traffic Light Control Through a Cooperative Multiagent System: A Simulation-Based Study
    authors: Francisco Guzmán, and Leonardo Garrido
    id: PADSSADSSSMSS2005_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 29-35
    booktitle: >
      Proceedings of the 2005 Agent-Directed Simulation Symposium (ADS05) at the 2005 Spring Simulation Multiconference (SpringSim'05)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      Present day traffic networks are unable to efficiently handle the daily car traffic through urban areas. We think that multiagent systems are an excellent way of doing microscopic simulation and thus provide possible solutions to the traffic control problem. In this paper, we present our simulation-based study to simulate traffic networks and optimize them via a multiagent cooperative system for traffic light control. This system simulates the traffic on an intersection, minimizing the time that each car has to wait in order to be served. Light agents can communicate each other in order to negotiate and share their light times. Our experimental results have shown how our approach can decrease the average car delay while the spawn probability is increased varying the service time and the number of traffic lights sets at a specific intersection. These results show important improvements using our multiagent light control system.
    bibtex: >
      @inproceedings{guzman:2005:towards,
       abstract = {Present day traffic networks are unable to efficiently handle the daily car traffic through urban areas. We think that multiagent systems are an excellent way of doing microscopic simulation and thus provide possible solutions to the traffic control problem. In this paper, we present our simulation-based study to simulate traffic networks and optimize them via a multiagent cooperative system for traffic light control. This system simulates the traffic on an intersection, minimizing the time that each car has to wait in order to be served. Light agents can communicate each other in order to negotiate and share their light times. Our experimental results have shown how our approach can decrease the average car delay while the spawn probability is increased varying the service time and the number of traffic lights sets at a specific intersection. These results show important improvements using our multiagent light control system.},
       address = {San Diego, California, USA},
       author = {Guzm{\'a}n, Francisco and Garrido, Leonardo},
       booktitle = {Proceedings of the 2005 Agent--Directed Simulation Symposium ({ADS05}) at the 2005 Spring Simulation Multiconference ({SpringSim'05})},
       month = {April},
       pages = {29--35},
       title = {Towards Traffic Light Control Through a Cooperative Multiagent System: A Simulation--Based Study},
       year = {2005}
      }
      
      
  -
    layout: paper
    paper-type: techreport #changes display of reference in paper list
    year: 2005
    selected: no #yes/no
    title: >
      Multiagent-Based Traffic Simulation
    authors: Francisco Guzmán, and Leonardo Garrido
    id: 2005_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    journal: Technical Report CSI-RI-005
    doc-url:  # e.g. papers/$ID.pdf
    bibtex: >
      @techreport{guzman:2005:multiagent,
       address = {Monterrey, Mexico},
       author = {Guzm{\'a}n, Francisco and Garrido, Leonardo},
       institution = {Tecnol{\'o}gico de Monterrey ({ITESM}), Monterrey, NL, M{\'e}xico},
       journal = {Technical Report CSI-RI-005},
       title = {Multiagent--Based Traffic Simulation},
       year = {2005}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2007
    selected: no #yes/no
    title: >
      Using Translation Paraphrases from Trilingual Corpora to Improve Phrase-Based Statistical Machine Translation: A Preliminary Report
    authors: Francisco Guzmán, and Leonardo Garrido
    id: SMICAIMICAI2007_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 163-172
    booktitle: >
      Sixth Mexican International Conference on Artificial Intelligence (MICAI'07).
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      Statistical methods have proven to be very effective when addressing linguistic problems, specially when dealing with Machine Translation. Nevertheless, Statistical Machine Translation effectiveness is limited to situations where large amounts of training data are available. Therefore, the broader the coverage of a SMT system is, the better the chances to get a reasonable output are. In this paper we propose a method to improve quality of translations of a phrase-based Machine Translation system by extending phrase-tables with the use of translation paraphrases learned from a third language. Our experiments were done translating from Spanish to English pivoting through French.
    bibtex: >
      @inproceedings{guzman:2007:using,
       abstract = {Statistical methods have proven to be very effective when addressing linguistic problems, specially when dealing with Machine Translation. Nevertheless, Statistical Machine Translation effectiveness is limited to situations where large amounts of training data are available. Therefore, the broader the coverage of a SMT system is, the better the chances to get a reasonable output are. In this paper we propose a method to improve quality of translations of a phrase-based Machine Translation system by extending phrase-tables with the use of translation paraphrases learned from a third language. Our experiments were done translating from Spanish to English pivoting through French.},
       address = {Aguascalientes, Mexico},
       author = {Guzm{\'a}n, Francisco and Garrido, Leonardo},
       booktitle = {Sixth Mexican International Conference on Artificial Intelligence {(MICAI'07)}.},
       month = {November},
       organization = {IEEE},
       pages = {163--172},
       title = {Using Translation Paraphrases from Trilingual Corpora to Improve Phrase-Based Statistical Machine Translation: A Preliminary Report},
       year = {2007}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2008
    selected: no #yes/no
    title: >
      Translation paraphrases in phrase-based machine translation
    authors: Francisco Guzmán, and Leonardo Garrido
    id: CLITPCICL2008_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 388-398
    booktitle: >
      Computational Linguistics and Intelligent Text Processing (CICLing'08)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      In this paper we present an analysis of a phrase-based machine translation methodology that integrates paraphrases obtained from an intermediary language (French) for translations between Spanish and English. The purpose of the research presented in this document is to find out how much extra information (i.e. improvements in translation quality) can be found when using Translation Paraphrases (TPs). In this document we present an extensive statistical analysis to support conclusions.
    bibtex: >
      @inproceedings{guzman:2008:translation,
       abstract = {In this paper we present an analysis of a phrase-based machine translation methodology that integrates paraphrases obtained from an intermediary language (French) for translations between Spanish and English. The purpose of the research presented in this document is to find out how much extra information (i.e. improvements in translation quality) can be found when using Translation Paraphrases (TPs). In this document we present an extensive statistical analysis to support conclusions.},
       address = {Haifa, Israel},
       author = {Guzm{\'a}n, Francisco and Garrido, Leonardo},
       booktitle = {Computational Linguistics and Intelligent Text Processing {(CICLing'08)}},
       month = {February},
       pages = {388--398},
       publisher = {Springer Berlin Heidelberg},
       title = {Translation paraphrases in phrase-based machine translation},
       year = {2008}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2009
    selected: no #yes/no
    title: >
      Reassessment of the Role of Phrase Extraction in PBSMT
    authors: Francisco Guzmán, Qin Gao, and Stephan Vogel
    id: TTMTSMTSXII2009_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    booktitle: >
      The Twelfth Machine Translation Summit (MTSummit XII)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      In this paper we study in detail the relation between word alignment and phrase extraction. First, we analyze different word alignments according to several characteristics and compare them to hand-aligned data. Secondly, we analyzed the phrase-pairs generated by these alignments. We observed that the number of unaligned words has a large impact on the characteristics of the phrase table. A manual evaluation of phrase pair quality showed that the increase in the number of unaligned words results in a lower quality. Finally, we present translation results from using the number of unaligned words as features from which we obtain up to 2BP of improvement.
    bibtex: >
      @inproceedings{guzman:2009:reassessment,
       abstract = {In this paper we study in detail the relation between word alignment and phrase extraction. First, we analyze different word alignments according to several characteristics and compare them to hand-aligned data. Secondly, we analyzed the phrase-pairs generated by these alignments. We observed that the number of unaligned words has a large impact on the characteristics of the phrase table. A manual evaluation of phrase pair quality showed that the increase in the number of unaligned words results in a lower quality. Finally, we present translation results from using the number of unaligned words as features from which we obtain up to 2BP of improvement.},
       address = {Ottawa, Canada},
       author = {Guzm{\'a}n, Francisco and Gao, Qin and Vogel, Stephan},
       booktitle = {The Twelfth Machine Translation Summit ({MTSummit XII})},
       month = {August},
       organization = {International Association for Machine Translation},
       title = {Reassessment of the Role of Phrase Extraction in {PBSMT}},
       year = {2009}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2010
    selected: no #yes/no
    title: >
      EMDC: a semi-supervised approach for word alignment
    authors: Qin Gao, Francisco Guzmán, and Stephan Vogel
    id: PICCLCOLING2010_Gao
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 349-357
    booktitle: >
      Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      This paper proposes a novel semi-supervised word alignment technique called EMDC that integrates discriminative and generative methods. A discriminative aligner is used to find high precision partial alignments that serve as constraints for a generative aligner which implements a constrained version of the EM algorithm. Experiments on small-size Chinese and Arabic tasks show consistent improvements on AER. We also experimented with moderate-size Chinese machine translation tasks and got an average of 0.5 point improvement on BLEU scores across five standard NIST test sets and four other test sets.
    bibtex: >
      @inproceedings{gao:2010:emdc,
       abstract = {This paper proposes a novel semi-supervised word alignment technique called EMDC that integrates discriminative and generative methods. A discriminative aligner is used to find high precision partial alignments that serve as constraints for a generative aligner which implements a constrained version of the EM algorithm. Experiments on small-size Chinese and Arabic tasks show consistent improvements on AER. We also experimented with moderate-size Chinese machine translation tasks and got an average of 0.5 point improvement on BLEU scores across five standard NIST test sets and four other test sets.},
       address = {Beijing, China},
       author = {Gao, Qin and Guzm{\'a}n, Francisco and Vogel, Stephan},
       booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics {(COLING 2010)}},
       organization = {Association for Computational Linguistics},
       pages = {349--357},
       title = {EMDC: a semi-supervised approach for word alignment},
       year = {2010}
      }
      
      
  -
    layout: paper
    paper-type: incollection #changes display of reference in paper list
    year: 2011
    selected: no #yes/no
    title: >
      Word Alignment Revisited
    authors: Francisco Guzmán, Qin Gao, Jan Niehues, and Stephan Vogel
    id: HNLPMT2011_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 164-175
    booktitle: >
      Handbook of Natural Language Processing and Machine Translation
    doc-url:  # e.g. papers/$ID.pdf
    bibtex: >
      @incollection{francisco:2011:word,
       author = {Guzm{\'a}n, Francisco and  Gao, Qin  and  Niehues, Jan  and Vogel,Stephan},
       booktitle = {Handbook of Natural Language Processing and Machine Translation},
       editor = {Olive, Joseph and Christianson, Caitlin and McCary, John},
       pages = {164--175},
       publisher = {Springer},
       title = {Word Alignment Revisited},
       year = {2011}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2012
    selected: no #yes/no
    title: >
      QCRI at WMT12: Experiments in Spanish-English and German-English Machine Translation of News Text
    authors: Francisco Guzmán, Preslav Nakov, Ahmed Thabet, and Stephan Vogel
    id: PSWSMTWMT2012_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 298-303
    booktitle: >
      Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT'12)
    doc-url: http://www.aclweb.org/anthology/W12-3136
    abstract: >
      We describe the systems developed by the team of the Qatar Computing Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English.
    bibtex: >
      @inproceedings{guzman-EtAl:2012:WMT,
       abstract = {We describe the systems developed by the team of the Qatar Computing Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English.},
       address = {Montr{\'e}al, Canada},
       author = {Guzm{\'a}n, Francisco  and  Nakov, Preslav  and  Thabet, Ahmed  and  Vogel, Stephan},
       booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation ({WMT'12})},
       link = {http://www.aclweb.org/anthology/W12-3136},
       month = {June},
       pages = {298--303},
       publisher = {Association for Computational Linguistics},
       title = {{QCRI} at {WMT}12: Experiments in Spanish-English and German-English Machine Translation of News Text},
       year = {2012}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2012
    selected: no #yes/no
    title: >
      Optimizing for Sentence-Level BLEU+1 Yields Short Translations
    authors: Preslav Nakov, Francisco Guzmán, and Stephan Vogel
    id: PICCLCOLING2012_Nakov
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 1979-1994
    booktitle: >
      Proceedings of the 24rd International Conference on Computational Linguistics (COLING 2012)
    doc-url: http://www.aclweb.org/anthology/C12-1121
    abstract: >
      We study a problem with pairwise ranking optimization (PRO): that it tends to yield too short translations. We find that this is partially due to the inadequate smoothing in PRO’s BLEU+1, which boosts the precision component of BLEU but leaves the brevity penalty unchanged, thus destroying the balance between the two, compared to BLEU. It is also partially due to PRO optimizing for a sentence-level score without a global view on the overall length, which introducing a bias towards short translations; we show that letting PRO optimize a corpus-level BLEU yields a perfect length. Finally, we find some residual bias due to the interaction of PRO with BLEU+1: such a bias does not exist for a version of MIRA with sentence-level BLEU+1. We propose several ways to fix the length problem of PRO, including smoothing the brevity penalty, scaling the effective reference length, grounding the precision component, and unclipping the brevity penalty, which yield sizable improvements in test BLEU on two Arabic-English datasets: IWSLT (+0.65) and NIST (+0.37).
    bibtex: >
      @inproceedings{nakov-guzman-vogel:2012:PAPERS,
       abstract = {We study a problem with pairwise ranking optimization (PRO): that it tends to yield too short translations. We find that this is partially due to the inadequate smoothing in PRO’s BLEU+1, which boosts the precision component of BLEU but leaves the brevity penalty unchanged, thus destroying the balance between the two, compared to BLEU. It is also partially due to PRO optimizing for a sentence-level score without a global view on the overall length, which introducing a bias towards short translations; we show that letting PRO optimize a corpus-level BLEU yields a perfect length. Finally, we find some residual bias due to the interaction of PRO with BLEU+1: such a bias does not exist for a version of MIRA with sentence-level BLEU+1. We propose several ways to fix the length problem of PRO, including smoothing the brevity penalty, scaling the effective reference length, grounding the precision component, and unclipping the brevity penalty, which yield sizable improvements in test BLEU on two Arabic-English datasets: IWSLT (+0.65) and NIST (+0.37).},
       address = {Mumbai, India},
       author = {Nakov, Preslav  and  Guzm{\'a}n, Francisco  and  Vogel, Stephan},
       booktitle = {Proceedings of the 24rd International Conference on Computational Linguistics {(COLING 2012)}},
       link = {http://www.aclweb.org/anthology/C12-1121},
       month = {December},
       pages = {1979--1994},
       publisher = {The {COLING} 2012 Organizing Committee},
       title = {Optimizing for Sentence-Level {BLEU}+1 Yields Short Translations},
       year = {2012}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2012
    selected: no #yes/no
    title: >
      Understanding the Performance of Statistical MT Systems: A Linear Regression Framework
    authors: Francisco Guzmán, and Stephan Vogel
    id: PICCLCOLING2012_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 1029-1044
    booktitle: >
      Proceedings of the 24rd International Conference on Computational Linguistics (COLING 2012)
    doc-url: http://www.aclweb.org/anthology/C12-1063
    abstract: >
      We present a framework for the analysis of Machine Translation performance. We use multivariate linear models to determine the impact of a wide range of features on translation performance. Our assumption is that variables that most contribute to predict translation performance are the key to understand the differences between good and bad translations. During training, we learn the regression parameters that better predict translation quality using a wide range of input features based on the translation model and the first-best translation hypotheses. We use a linear regression with regularization. Our results indicate that with regularized linear regression, we can achieve higher levels of correlation between our predicted values and the actual values of the quality metrics. Our analysis shows that the performance for in-domain data is largely dependent on the characteristics of the translation model. On the other hand, out-of domain data can benefit from better reordering strategies.
    bibtex: >
      @inproceedings{guzman-vogel:2012:PAPERS,
       abstract = {We present a framework for the analysis of Machine Translation performance. We use multivariate linear models to determine the impact of a wide range of features on translation performance. Our assumption is that variables that most contribute to predict translation performance are the key to understand the differences between good and bad translations. During training, we learn the regression parameters that better predict translation quality using a wide range of input features based on the translation model and the first-best translation hypotheses. We use a linear regression with regularization. Our results indicate that with regularized linear regression, we can achieve higher levels of correlation between our predicted values and the actual values of the quality metrics. Our analysis shows that the performance for in-domain data is largely dependent on the characteristics of the translation model. On the other hand, out-of domain data can benefit from better reordering strategies.},
       address = {Mumbai, India},
       author = {Guzm{\'a}n, Francisco  and  Vogel, Stephan},
       booktitle = {Proceedings of the 24rd International Conference on Computational Linguistics {(COLING 2012)}},
       link = {http://www.aclweb.org/anthology/C12-1063},
       month = {December},
       pages = {1029--1044},
       title = {Understanding the Performance of Statistical {MT} Systems: A Linear Regression Framework},
       year = {2012}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2013
    selected: no #yes/no
    title: >
      A Tale about PRO and Monsters
    authors: Preslav Nakov, Francisco Guzmán, and Stephan Vogel
    id: PAMACLACL2013_Nakov
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 12-17
    booktitle: >
      Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL'13)
    doc-url: http://www.aclweb.org/anthology/P13-2003
    abstract: >
      While experimenting with tuning on long sentences, we made an unexpected discovery: that PRO falls victim to monsters -overly long negative examples with very low BLEU+1 scores, which are unsuitable for learning and can cause testing BLEU to drop by several points absolute. We propose several effective ways to address the problem, using length- and BLEU+1- based cut-offs, outlier filters, stochastic sampling, and random acceptance. The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved test-time BLEU scores. Thus, we recommend them to anybody using PRO, monster-believer or not.
    bibtex: >
      @inproceedings{nakov-guzman-vogel:2013:Short,
       abstract = {While experimenting with tuning on long sentences, we made an unexpected discovery: that PRO falls victim to monsters -overly long negative examples with very low BLEU+1 scores, which are unsuitable for learning and can cause testing BLEU to drop by several points absolute. We propose several effective ways to address the problem, using length- and BLEU+1- based cut-offs, outlier filters, stochastic sampling, and random acceptance. The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved test-time BLEU scores. Thus, we recommend them to anybody using PRO, monster-believer or not.},
       address = {Sofia, Bulgaria},
       author = {Nakov, Preslav  and  Guzm{\'a}n, Francisco  and  Vogel, Stephan},
       booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ({ACL'13})},
       link = {http://www.aclweb.org/anthology/P13-2003},
       month = {August},
       pages = {12--17},
       title = {A Tale about {PRO} and Monsters},
       year = {2013}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2013
    selected: no #yes/no
    title: >
      Parameter Optimization for Statistical Machine Translation: It Pays to Learn from Hard Examples
    authors: Preslav Nakov, Fahad Al Obaidli, Francisco Guzmán, and Stephan Vogel
    id: PICRANLPRANLP2013_Nakov
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    booktitle: >
      Proceedings of the International Conference Recent Advances in Natural Language Processing (RANLP'13)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      Research on statistical machine translation has focused on particular translation directions, typically with English as the target language, e.g., from Arabic to English. When we reverse the translation direction, the multiple reference translations turn into multiple possible inputs, which offers both challenges and opportunities. We propose and evaluate several strategies for making use of these multiple inputs: (a) select one of the datasets, (b) select the best input for each sentence, and (c) synthesize an input for each sentence by fusing the available inputs. Surprisingly, we find out that it is best to tune on the hardest available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT.
    bibtex: >
      @inproceedings{nakov:2013:parameter,
       abstract = {Research on statistical machine translation has focused on particular translation directions, typically with English as the target language, e.g., from Arabic to English. When we reverse the translation direction, the multiple reference translations turn into multiple possible inputs, which offers both challenges and opportunities. We propose and evaluate several strategies for making use of these multiple inputs: (a) select one of the datasets, (b) select the best input for each sentence, and (c) synthesize an input for each sentence by fusing the available inputs. Surprisingly, we find out that it is best to tune on the hardest available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT.},
       author = {Nakov, Preslav and Al Obaidli, Fahad and Guzm{\'a}n, Francisco and Vogel, Stephan},
       booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing ({RANLP}'13)},
       month = {September},
       title = {Parameter Optimization for Statistical Machine Translation: It Pays to Learn from Hard Examples},
       year = {2013}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2013
    selected: no #yes/no
    title: >
      QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic Spoken Language Translation
    authors: Hassan Sajjad, Francisco Guzmán, Preslav Nakov, Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli, and Stephan Vogel
    id: PIWSLTIWSLT2013_Sajjad
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    booktitle: >
      Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT'13)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      We describe the Arabic-English and English-Arabic statistical machine translation systems developed by the Qatar Computing Research Institute for the IWSLT’2013 evaluation campaign on spoken language translation. We used one phrase-based and two hierarchical decoders, exploring various settings thereof. We further experimented with three domain adaptation methods, and with various Arabic word segmentation schemes. Combining the output of several systems yielded a gain of up to 3.4 BLEU points over the baseline. Here we also describe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign.
    bibtex: >
      @inproceedings{sajjad:2013:qcri,
       abstract = {We describe the Arabic-English and English-Arabic statistical machine translation systems developed by the Qatar Computing Research Institute for the IWSLT’2013 evaluation campaign on spoken language translation. We used one phrase-based and two hierarchical decoders, exploring various settings thereof. We further experimented with three domain adaptation methods, and with various Arabic word segmentation schemes. Combining the output of several systems yielded a gain of up to 3.4 BLEU points over the baseline. Here we also describe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign.},
       address = {Heidelberg, Germany},
       author = {Sajjad, Hassan and Guzm{\'a}n, Francisco and Nakov, Preslav and Abdelali, Ahmed and Murray, Kenton and Al Obaidli, Fahad and Vogel, Stephan},
       booktitle = {Proceedings of the 10th International Workshop on Spoken Language Translation {(IWSLT'13)}},
       month = {December},
       title = {{QCRI} at {IWSLT} 2013: Experiments in Arabic-English and English-Arabic Spoken Language Translation},
       volume = {13},
       year = {2013}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2013
    selected: no #yes/no
    title: >
      The AMARA Corpus: Building Resources for Translating the Web's Educational Content
    authors: Francisco Guzmán, Hassan Sajjad, Ahmed Abdelali, and Stephan Vogel
    id: PIWSLTIWSLT2013_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    booktitle: >
      Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT'13)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      In this paper, we introduce a new parallel corpus of subtitles of educational videos: the AMARA corpus for online educational content. We crawl a multilingual collection community generated subtitles, and present the results of processing the Arabic–English portion of the data, which yields a parallel corpus of about 2.6M Arabic and 3.9M English words. We explore different approaches to align the segments, and extrinsically evaluate the resulting parallel corpus on the standard TED-talks tst-2010. We observe that the data can be successfully used for this task, and also observe an absolute improvement of 1.6 BLEU when it is used in combination with TED data. Finally, we analyze some of the specific challenges when translating the educational content.
    bibtex: >
      @inproceedings{guzman:2013:amara,
       abstract = {In this paper, we introduce a new parallel corpus of subtitles of educational videos: the AMARA corpus for online educational content. We crawl a multilingual collection community generated subtitles, and present the results of processing the Arabic–English portion of the data, which yields a parallel corpus of about 2.6M Arabic and 3.9M English words. We explore different approaches to align the segments, and extrinsically evaluate the resulting parallel corpus on the standard TED-talks tst-2010. We observe that the data can be successfully used for this task, and also observe an absolute improvement of 1.6 BLEU when it is used in combination with TED data. Finally, we analyze some of the specific challenges when translating the educational content.},
       address = {Heidelberg, Germany},
       author = {Guzm{\'a}n, Francisco and Sajjad, Hassan and Abdelali, Ahmed and Vogel, Stephan},
       booktitle = {Proceedings of the 10th International Workshop on Spoken Language Translation {(IWSLT'13})},
       month = {December},
       title = {The {AMARA} Corpus: Building Resources for Translating the Web's Educational Content},
       volume = {13},
       year = {2013}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2014
    selected: no #yes/no
    title: >
      The AMARA Corpus: Building parallel language resources for the educational domain
    authors: Ahmed Abdelali, Francisco Guzmán, Hassan Sajjad, and Stephan Vogel
    id: PNICLRELREC2014_Abdelali
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    booktitle: >
      Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      This paper presents the AMARA corpus of on-line educational content: a new parallel corpus of educational video subtitles, multilingually aligned for 20 languages, i.e. 20 monolingual corpora and 190 parallel corpora. This corpus includes both resource-rich languages such as English and Arabic, and resource-poor languages such as Hindi and Thai. In this paper, we describe the gathering, validation, and preprocessing of a large collection of parallel, community-generated subtitles. Furthermore, we describe the methodology used to prepare the data for Machine Translation tasks. Additionally, we provide a document-level, jointly aligned development and test sets for 14 language pairs, designed for tuning and testing Machine Translation systems. We provide baseline results for these tasks, and highlight some of the challenges we face when building machine translation systems for educational content.
    bibtex: >
      @inproceedings{abdelali:2014:amara,
       abstract = {This paper presents the AMARA corpus of on-line educational content: a new parallel corpus of educational video subtitles, multilingually aligned for 20 languages, i.e. 20 monolingual corpora and 190 parallel corpora. This corpus includes both resource-rich languages such as English and Arabic, and resource-poor languages such as Hindi and Thai. In this paper, we describe the gathering, validation, and preprocessing of a large collection of parallel, community-generated subtitles. Furthermore, we describe the methodology used to prepare the data for Machine Translation tasks. Additionally, we provide a document-level, jointly aligned development and test sets for 14 language pairs, designed for tuning and testing Machine Translation systems. We provide baseline results for these tasks, and highlight some of the challenges we face when building machine translation systems for educational content.},
       address = {Reykjavik, Iceland},
       author = {Abdelali, Ahmed and Guzm{\'a}n, Francisco and Sajjad, Hassan and Vogel, Stephan},
       booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
       month = {april},
       title = {The {AMARA} Corpus: Building parallel language resources for the educational domain},
       year = {2014}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2014
    selected: no #yes/no
    title: >
      Amara: A Sustainable, Global Solution for Accessibility, Powered by Communities of Volunteers
    authors: Dean Jansen, Aleli Alcala, and Francisco Guzmán
    id: UAHCIDAAP2014_Jansen
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 401-411
    booktitle: >
      Universal Access in Human-Computer Interaction. Design for All and Accessibility Practice
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      In this paper, we present the main features of the Amara project, and its impact on the accessibility landscape with the use of innovative technology. We also show the effectiveness of volunteer communities in addressing large subtitling and translation tasks, that accompany the ever-growing amounts of online video content. Furthermore, we present two different applications for the platform. First, we examine the growing interest of organizations to build their own subtitling communities. Second, we present how the community-generated material can be used to advance the state-of-the-art of research in fields such as Statistical Machine Translation with focus on educational translation. We provide examples on how both tasks can be achieved successfully.
    bibtex: >
      @inproceedings{jansen:2014:amara,
       abstract = {In this paper, we present the main features of the Amara project, and its impact on the accessibility landscape with the use of innovative technology. We also show the effectiveness of volunteer communities in addressing large subtitling and translation tasks, that accompany the ever-growing amounts of online video content. Furthermore, we present two different applications for the platform. First, we examine the growing interest of organizations to build their own subtitling communities. Second, we present how the community-generated material can be used to advance the state-of-the-art of research in fields such as Statistical Machine Translation with focus on educational translation. We provide examples on how both tasks can be achieved successfully.},
       address = {Heraklion, Greece},
       author = {Jansen, Dean and Alcala, Aleli and Guzm{\'a}n, Francisco},
       booktitle = {Universal Access in Human-Computer Interaction. Design for All and Accessibility Practice},
       month = {June},
       pages = {401--411},
       publisher = {Springer International Publishing},
       title = {Amara: A Sustainable, Global Solution for Accessibility, Powered by Communities of Volunteers},
       year = {2014}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2014
    selected: no #yes/no
    title: >
      Using Discourse Structure Improves Machine Translation Evaluation
    authors: Francisco Guzmán, Shafiq Joty, Lluís Màrquez, and Preslav Nakov
    id: PAMACLACL2014_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 687-698
    booktitle: >
      Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL'14)
    doc-url: http://www.aclweb.org/anthology/P/P14/P14-1065
    abstract: >
      We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segmentand at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.
    bibtex: >
      @inproceedings{guzman-EtAl:2014:P14-1,
       abstract = {We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segmentand at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.},
       address = {Baltimore, Maryland, USA},
       author = {Guzm\'{a}n, Francisco  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav},
       booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ({ACL}'14)},
       link = {http://www.aclweb.org/anthology/P/P14/P14-1065},
       month = {June},
       pages = {687--698},
       publisher = {Association for Computational Linguistics},
       title = {Using Discourse Structure Improves Machine Translation Evaluation},
       year = {2014}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2014
    selected: no #yes/no
    title: >
      DiscoTK: Using Discourse Structure for Machine Translation Evaluation
    authors: Shafiq Joty, Francisco Guzmán, Lluís Màrquez, and Preslav Nakov
    id: PNWSMTWMT2014_Joty
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 402-408
    booktitle: >
      Proceedings of the Ninth Workshop on Statistical Machine Translation (WMT'14)
    doc-url: http://www.aclweb.org/anthology/W/W14/W14-3352
    abstract: >
      We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.
    bibtex: >
      @inproceedings{joty-EtAl:2014:W14-33,
       abstract = {We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.},
       address = {Baltimore, Maryland, USA},
       author = {Joty, Shafiq  and  Guzm\'{a}n, Francisco  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav},
       booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation ({WMT}'14)},
       link = {http://www.aclweb.org/anthology/W/W14/W14-3352},
       month = {June},
       pages = {402--408},
       publisher = {Association for Computational Linguistics},
       title = {DiscoTK: Using Discourse Structure for Machine Translation Evaluation},
       year = {2014}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2014
    selected: no #yes/no
    title: >
      Learning to Differentiate Better from Worse Translations
    authors: Francisco Guzmán, Shafiq Joty, Lluís Màrquez, Alessandro Moschitti, Preslav Nakov, and Massimo Nicosia
    id: PCEMNLPEMNLP2014_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 214-220
    booktitle: >
      Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP'14)
    doc-url: http://www.aclweb.org/anthology/D14-1027
    abstract: >
      We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically. The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated.
    bibtex: >
      @inproceedings{guzman-EtAl:2014:EMNLP2014,
       abstract = {We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically. The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated.},
       address = {Doha, Qatar},
       author = {Guzm\'{a}n, Francisco  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Moschitti, Alessandro  and  Nakov, Preslav  and  Nicosia, Massimo},
       booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}'14)},
       link = {http://www.aclweb.org/anthology/D14-1027},
       month = {October},
       pages = {214--220},
       publisher = {Association for Computational Linguistics},
       title = {Learning to Differentiate Better from Worse Translations},
       year = {2014}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2015
    selected: no #yes/no
    title: >
      Pairwise Neural Machine Translation Evaluation
    authors: Francisco Guzmán, Shafiq Joty, Lluís Màrquez, and Preslav Nakov
    id: PAMACLTIJCAFNLPACL2015_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 805-814
    booktitle: >
      Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing (ACL'15)
    doc-url: http://www.aclweb.org/anthology/P15-1078
    abstract: >
      We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.
    bibtex: >
      @inproceedings{guzman2015-ACL,
       abstract = {We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.},
       address = {Beijing, China},
       author = {Guzm\'{a}n, Francisco  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav},
       booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian
      Federation of Natural Language Processing ({ACL}'15)},
       link = {http://www.aclweb.org/anthology/P15-1078},
       month = {July},
       pages = {805--814},
       publisher = {Association for Computational Linguistics},
       title = {Pairwise Neural Machine Translation Evaluation},
       year = {2015}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2015
    selected: no #yes/no
    title: >
      QAT$^2$-The QCRI Advanced Transcription and Translation System
    authors: Ahmed Abdelali, Ahmed Ali, Francisco Guzmán, Felix Stahlberg, Stephan Vogel, and Yifan Zhang
    id: PACISCA2015_Abdelali
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    booktitle: >
      Proceedings of the 16th Annual Conference of the International Speech Communication Association
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      QAT2 is a multimedia content translation web service developed by QCRI to help content provider to reach audiences and viewers speaking different languages. It is built with  open source technologies such as KALDI, Moses and MaryTTS, to provide a complete translation experience for web users. It translates text content in its original format, and pro- duce translated videos with speech-to-speech translation. The result is a complete native language experience for end users on foreign language websites. The system currently supports translation from Arabic to English.
    bibtex: >
      @inproceedings{abdelali2015-INTERSPEECH,
       abstract = {QAT2 is a multimedia content translation web service developed by QCRI to help content provider to reach audiences and viewers speaking different languages. It is built with  open source technologies such as KALDI, Moses and MaryTTS, to provide a complete translation experience for web users. It translates text content in its original format, and pro- duce translated videos with speech-to-speech translation. The result is a complete native language experience for end users on foreign language websites. The system currently supports translation from Arabic to English.},
       address = {Dresden, Germany},
       author = {Abdelali, Ahmed and Ali, Ahmed and Guzm{\'a}n, Francisco and Stahlberg, Felix and Vogel, Stephan and Zhang, Yifan},
       booktitle = {Proceedings of the 16th Annual Conference of the International Speech Communication Association},
       title = {{QAT}$^2$--The {QCRI} Advanced Transcription and Translation System},
       year = {2015}
      }
      
      
  -
    layout: paper
    paper-type: proceedings #changes display of reference in paper list
    year: 2015
    selected: no #yes/no
    title: >
      The QCN Egyptian Arabic to English Statistical Machine Translation System for NIST OpenMT’2015
    authors: Hassan Sajjad, Nadir Durrani, Francisco Guzmán, Preslav Nakov, Ahmed Abdelali, Stephan Vogel, Wael Salloum, Ahmed El Kholy, and Nizar Habash
    id: PNISTOMTEW2015_Sajjad
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    booktitle: >
      Proceedings of the NIST Open Machine Translation Evaluation Workshop
    doc-url:  # e.g. papers/$ID.pdf
    abstract: >
      The paper describes the Egyptian Arabic-to-English statistical machine translation (SMT) system that the QCRI-Columbia-NYUAD (QCN) group submitted to the NIST OpenMT'2015 competition. The competition focused on informal dialectal Arabic, as used in SMS, chat, and speech. Thus, our efforts focused on processing and standardizing Arabic, e.g., using tools such as 3arrib and MADAMIRA. We further trained a phrase-based SMT system using state-of-the-art features and components such as operation sequence model, class-based language model, sparse features, neural network joint model, genre-based hierarchically-interpolated language model, unsupervised transliteration mining, phrase-table merging, and hypothesis combination. Our system ranked second on all three genres.
    bibtex: >
      @proceedings{sajjad2015-NIST,
       abstract = {The paper describes the Egyptian Arabic-to-English statistical machine translation (SMT) system that the QCRI-Columbia-NYUAD (QCN) group submitted to the NIST OpenMT'2015 competition. The competition focused on informal dialectal Arabic, as used in SMS, chat, and speech. Thus, our efforts focused on processing and standardizing Arabic, e.g., using tools such as 3arrib and MADAMIRA. We further trained a phrase-based SMT system using state-of-the-art features and components such as operation sequence model, class-based language model, sparse features, neural network joint model, genre-based hierarchically-interpolated language model, unsupervised transliteration mining, phrase-table merging, and hypothesis combination. Our system ranked second on all three genres.},
       author = {Sajjad, Hassan and Durrani, Nadir and Guzmán, Francisco and Nakov, Preslav and Abdelali, Ahmed and Vogel, Stephan and Salloum, Wael and El Kholy, Ahmed and Habash, Nizar},
       booktitle = {Proceedings of the NIST Open Machine Translation Evaluation Workshop},
       publisher = {NIST},
       title = {The QCN Egyptian Arabic to English Statistical Machine Translation System for NIST OpenMT’2015},
       year = {2015}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2015
    selected: no #yes/no
    title: >
      Analyzing Optimization for Statistical Machine Translation: MERT Learns Verbosity, PRO Learns Length
    authors: Francisco Guzmán, Preslav Nakov, and Stephan Vogel
    id: PNCCNLL2015_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 62-72
    booktitle: >
      Proceedings of the Nineteenth Conference on Computational Natural Language Learning
    doc-url: http://www.aclweb.org/anthology/K15-1007
    abstract: >
      We study the impact of source length and verbosity of the tuning dataset on the performance of parameter optimizers such as MERT and PRO for statistical machine translation. In particular, we test whether the verbosity of the resulting translations can be modified by varying the length or the verbosity of the tuning sentences. We find that MERT learns the tuning set verbosity very well, while PRO is sensitive to both the verbosity and the length of the source sentences in the tuning set; yet, overall PRO learns best from high-verbosity tuning datasets. Given these dependencies, and potentially some other such as amount of reordering, number of unknown words, syntactic complexity, and evaluation measure, to mention just a few, we argue for the need of controlled evaluation scenarios, so that the selection of tuning set and optimization strategy does not overshadow scientific advances in modeling or decoding. In the mean time, until we develop such controlled scenarios, we recommend using PRO with a large verbosity tuning set, which, in our experiments, yields highest BLEU across datasets and language pairs.
    bibtex: >
      @inproceedings{guzman-nakov-vogel:2015:CoNLL,
       abstract = {We study the impact of source length and verbosity of the tuning dataset on the performance of parameter optimizers such as MERT and PRO for statistical machine translation. In particular, we test whether the verbosity of the resulting translations can be modified by varying the length or the verbosity of the tuning sentences. We find that MERT learns the tuning set verbosity very well, while PRO is sensitive to both the verbosity and the length of the source sentences in the tuning set; yet, overall PRO learns best from high-verbosity tuning datasets.
      Given these dependencies, and potentially some other such as amount of reordering, number of unknown words, syntactic complexity, and evaluation measure, to mention just a few, we argue for the need of controlled evaluation scenarios, so that the selection of tuning set and optimization strategy does not overshadow scientific advances in modeling or decoding. In the mean time, until we develop such controlled scenarios, we recommend using PRO with a large verbosity tuning set, which, in our experiments, yields highest BLEU across datasets and language pairs.},
       address = {Beijing, China},
       author = {Guzm\'{a}n, Francisco  and  Nakov, Preslav  and  Vogel, Stephan},
       booktitle = {Proceedings of the Nineteenth Conference on Computational Natural Language Learning},
       link = {http://www.aclweb.org/anthology/K15-1007},
       month = {July},
       pages = {62--72},
       publisher = {Association for Computational Linguistics},
       title = {Analyzing Optimization for Statistical Machine Translation: MERT Learns Verbosity, PRO Learns Length},
       year = {2015}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2015
    selected: no #yes/no
    title: >
      How do Humans Evaluate Machine Translation
    authors: Francisco Guzmán, Ahmed Abdelali, Irina Temnikova, Hassan Sajjad, and Stephan Vogel
    id: PTWSMT2015_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 457-466
    booktitle: >
      Proceedings of the Tenth Workshop on Statistical Machine Translation
    doc-url: http://aclweb.org/anthology/W15-3059
    abstract: >
      In this paper, we take a closer look at the MT evaluation process from a glass-box perspective using eye-tracking. We analyze two aspects of the evaluation task: the background of evaluators (monolingual or bilingual) and the sources of information available, and we evaluate them using time and consistency as criteria. Our findings show that monolinguals are slower but more consistent than bilinguals, especially when only target language information is available. When exposed to various sources of information, evaluators in general take more time and in the case of monolinguals, there is a drop in consistency. Our findings suggest that to have consistent and cost effective MT evaluations, it is better to use monolinguals with only target language information.
    bibtex: >
      @inproceedings{guzman-EtAl:2015:WMT,
       abstract = {In this paper, we take a closer look at the MT evaluation process from a glass-box perspective using eye-tracking. We analyze two aspects of the evaluation task: the background of evaluators (monolingual or bilingual) and the sources of information available, and we evaluate them using time and consistency as criteria. Our findings show that monolinguals are slower but more consistent than bilinguals, especially when only target language information is available. When exposed to various sources of information, evaluators in general take more time and in the case of monolinguals, there is a drop in consistency. Our findings suggest that to have consistent and cost effective MT evaluations, it is better to use monolinguals with only target language information.},
       address = {Lisbon, Portugal},
       author = {Guzm\'{a}n, Francisco  and  Abdelali, Ahmed  and  Temnikova, Irina  and  Sajjad, Hassan  and  Vogel, Stephan},
       booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
       link = {http://aclweb.org/anthology/W15-3059},
       month = {September},
       pages = {457--466},
       publisher = {Association for Computational Linguistics},
       title = {How do Humans Evaluate Machine Translation},
       year = {2015}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      Eyes Don't Lie: Predicting Machine Translation Quality Using Eye Movement
    authors: Hassan Sajjad, Francisco Guzmán, Nadir Durrani, Ahmed Abdelali, Houda Bouamor, Irina Temnikova, and Stephan Vogel
    id: PCNACACLHLT2016_Sajjad
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 1082-1088
    booktitle: >
      Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
    doc-url: http://www.aclweb.org/anthology/N16-1125
    bibtex: >
      @inproceedings{sajjad-EtAl:2016:N16-1,
       address = {San Diego, California},
       author = {Sajjad, Hassan  and  Guzm\'{a}n, Francisco  and  Durrani, Nadir  and  Abdelali, Ahmed  and  Bouamor, Houda  and  Temnikova, Irina  and  Vogel, Stephan},
       booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
       link = {http://www.aclweb.org/anthology/N16-1125},
       month = {June},
       pages = {1082--1088},
       publisher = {Association for Computational Linguistics},
       title = {Eyes Don't Lie: Predicting Machine Translation Quality Using Eye Movement},
       year = {2016}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      iAppraise: A Manual Machine Translation Evaluation Environment Supporting Eye-tracking
    authors: Ahmed Abdelali, Nadir Durrani, and Francisco Guzmán
    id: PCNACACLD2016_Abdelali
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 17-21
    booktitle: >
      Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations
    doc-url: http://www.aclweb.org/anthology/N16-3004
    bibtex: >
      @inproceedings{abdelali-durrani-guzman:2016:N16-3,
       address = {San Diego, California},
       author = {Abdelali, Ahmed  and  Durrani, Nadir  and  Guzm\'{a}n, Francisco},
       booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations},
       link = {http://www.aclweb.org/anthology/N16-3004},
       month = {June},
       pages = {17--21},
       publisher = {Association for Computational Linguistics},
       title = {iAppraise: A Manual Machine Translation Evaluation Environment Supporting Eye-tracking},
       year = {2016}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      Machine Translation Evaluation Meets Community Question Answering
    authors: Francisco Guzmán, Lluís Màrquez, and Preslav Nakov
    id: PAMACL2016_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 460-466
    booktitle: >
      Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
    doc-url: http://anthology.aclweb.org/P16-2075
    bibtex: >
      @inproceedings{guzman-marquez-nakov:2016:P16-2,
       address = {Berlin, Germany},
       author = {Guzm\'{a}n, Francisco  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav},
       booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
       link = {http://anthology.aclweb.org/P16-2075},
       month = {August},
       pages = {460--466},
       publisher = {Association for Computational Linguistics},
       title = {Machine Translation Evaluation Meets Community Question Answering},
       year = {2016}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      MTE-NN at SemEval-2016 Task 3: Can Machine Translation Evaluation Help Community Question Answering?
    authors: Francisco Guzmán, Preslav Nakov, and Lluís Màrquez
    id: PIWSESE2016_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 887-895
    booktitle: >
      Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)
    doc-url: http://www.aclweb.org/anthology/S16-1137
    bibtex: >
      @inproceedings{guzman-nakov-marquez:2016:SemEval,
       address = {San Diego, California},
       author = {Guzm\'{a}n, Francisco  and  Nakov, Preslav  and  M\`{a}rquez, Llu\'{i}s},
       booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
       link = {http://www.aclweb.org/anthology/S16-1137},
       month = {June},
       pages = {887--895},
       publisher = {Association for Computational Linguistics},
       title = {MTE-NN at SemEval-2016 Task 3: Can Machine Translation Evaluation Help Community Question Answering?},
       year = {2016}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      It Takes Three to Tango: Triangulation Approach to Answer Ranking in Community Question Answering
    authors: Preslav Nakov, Lluís Màrquez, and Francisco Guzmán
    id: PCEMNLP2016_Nakov
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 1586-1597
    booktitle: >
      Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing
    doc-url: https://aclweb.org/anthology/D16-1165
    bibtex: >
      @inproceedings{nakov-marquez-guzman:2016:EMNLP2016,
       address = {Austin, Texas},
       author = {Nakov, Preslav  and  M\`{a}rquez, Llu\'{i}s  and  Guzm\'{a}n, Francisco},
       booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
       link = {https://aclweb.org/anthology/D16-1165},
       month = {November},
       pages = {1586--1597},
       publisher = {Association for Computational Linguistics},
       title = {It Takes Three to Tango: Triangulation Approach to Answer Ranking in Community Question Answering},
       year = {2016}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      Normalizing Mathematical Expressions to Improve the Translation of Educational Content
    authors: Wajdi Zaghouani, Ahmed Abdelali, Francisco Guzmán, and Hassan Sajjad
    id: PWSMT2016_Zaghouani
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 20-27
    booktitle: >
      Proceedings of the Workshop on Semitic Machine Translation
    doc-url: http://www.aclweb.org/anthology/W/W05/W05-0204
    bibtex: >
      @inproceedings{zaghouani-EtAl:2016:SeMaT,
       address = {Austin, Texas},
       author = {Zaghouani, Wajdi  and  Abdelali, Ahmed  and  Guzm\'{a}n, Francisco  and  Sajjad, Hassan},
       booktitle = {Proceedings of the Workshop on Semitic Machine Translation},
       link = {http://www.aclweb.org/anthology/W/W05/W05-0204},
       month = {November},
       pages = {20--27},
       publisher = {Association for Computational Linguistics},
       title = {Normalizing Mathematical Expressions to Improve the Translation of Educational Content},
       year = {2016}
      }
      
      
  -
    layout: paper
    paper-type: inproceedings #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      Machine Translation Evaluation for Arabic using Morphologically-enriched Embeddings
    authors: Francisco Guzmán, Houda Bouamor, Ramy Baly, and Nizar Habash
    id: PCOLINGICCLTP2016_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    pages: 1398-1408
    booktitle: >
      Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers
    doc-url: http://aclweb.org/anthology/C16-1132
    abstract: >
      Evaluation of machine translation (MT) into morphologically rich languages (MRL) has not been well studied despite posing many challenges. In this paper, we explore the use of embeddings obtained from different levels of lexical and morpho-syntactic linguistic analysis and show that they improve MT evaluation into an MRL. Specifically we report on Arabic, a language with complex and rich morphology. Our results show that using a neural-network model with different input representations produces results that clearly outperform the state-of-the-art for MT evaluation into Arabic, by almost over 75% increase in correlation with human judgments on pairwise MT evaluation quality task. More importantly, we demonstrate the usefulness of morpho-syntactic representations to model sentence similarity for MT evaluation and address complex linguistic phenomena of Arabic.
    bibtex: >
      @inproceedings{guzman-EtAl:2016:COLING,
       abstract = {Evaluation of machine translation (MT) into morphologically rich languages
      (MRL) has not been
      well studied despite posing many challenges. In this paper, we explore the use
      of embeddings obtained from different levels of lexical and morpho-syntactic
      linguistic analysis and show that they improve MT evaluation into an MRL.
      Specifically we report on Arabic, a language with complex and rich morphology.
      Our results show that using a neural-network model with different input
      representations produces results that clearly outperform the state-of-the-art
      for MT evaluation into Arabic, by almost over 75% increase in correlation with
      human judgments on pairwise MT evaluation quality task. More importantly, we
      demonstrate the usefulness of morpho-syntactic representations to model
      sentence similarity for MT evaluation and address complex linguistic phenomena
      of Arabic.},
       address = {Osaka, Japan},
       author = {Guzm\'{a}n, Francisco  and  Bouamor, Houda  and  Baly, Ramy  and  Habash, Nizar},
       booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
       link = {http://aclweb.org/anthology/C16-1132},
       month = {December},
       pages = {1398--1408},
       publisher = {The COLING 2016 Organizing Committee},
       title = {Machine Translation Evaluation for Arabic using Morphologically-enriched Embeddings},
       year = {2016}
      }
      
      
  -
    layout: paper
    paper-type: article #changes display of reference in paper list
    year: 2016
    selected: no #yes/no
    title: >
      Machine translation evaluation with neural networks
    authors: Francisco Guzmán, Shafiq Joty, Lluís Màrquez, and Preslav Nakov
    id: 2016_Guzman
    img: #image_id to be found in img/paper/ID.jpg
    slides: # e.g. media/$ID.pptx 
    code: #e.g. github.com/project
    errata: #if you have errata, insert here
    venue: #for CV e.g. book[chapters], conference[journal],  workshop[demo], techreport
    journal: Computer Speech & Language
    doc-url: http://www.sciencedirect.com/science/article/pii/S0885230816301693
    abstract: >
      Abstract We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is embedded into compact distributed vector representations, and fed into a multi-layer neural network that models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses. We experiment with the benchmark datasets from the \WMT\ Metrics shared task, on which we obtain the best results published so far, with the basic network configuration. We also perform a series of experiments to analyze and understand the contribution of the different components of the network. We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled with convolutional and recurrent neural networks. In summary, the proposed framework is flexible and generalizable, allows for efficient learning and scoring, and provides an \MT\ evaluation metric that correlates with human judgments, and is on par with the state of the art.
    bibtex: >
      @article{GuzmanCSL2016,
       abstract = {Abstract We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation.
      In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses
      is embedded into compact distributed vector representations, and fed into a multi-layer neural network that
      models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses.
      We experiment with the benchmark datasets from the \{WMT\} Metrics shared task, on which we obtain the best results published so far, with the basic network configuration.
      We also perform a series of experiments to analyze and understand the contribution of the different components of the network.
      We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled with
      convolutional and recurrent neural networks. In summary, the proposed framework is flexible and generalizable, allows for efficient learning and scoring, and provides an \{MT\} evaluation metric that correlates with human judgments, and is on par with the state of the art.},
       author = {Guzm\'{a}n,Francisco  and Joty, Shafiq  and Màrquez,Lluís  and Nakov, Preslav},
       doi = {http://dx.doi.org/10.1016/j.csl.2016.12.005},
       issn = {0885-2308},
       journal = {Computer Speech & Language},
       link = {http://www.sciencedirect.com/science/article/pii/S0885230816301693},
       title = {Machine translation evaluation with neural networks},
       year = {2016}
      }
      
      
