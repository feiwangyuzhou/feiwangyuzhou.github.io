---
abstract: 'Morphological inflection is a process of word formation where base words
  are modified to express different grammatical categories such as tense, case, voice,
  person, or number. World Englishes, such as Colloquial Singapore English (CSE) and
  African American Vernacular English (AAVE), differ from Standard English dialects
  in inflection use. Although comprehension by human readers is usually unimpaired
  by non-standard inflection use, NLP systems are not so robust. We introduce a new
  Base-Inflection Encoding of English text that is achieved by combining linguistic
  and statistical techniques. Fine-tuning pre-trained NLP models for downstream tasks
  under this novel encoding achieves robustness to non-standard inflection use while
  maintaining performance on Standard English examples. Models using this encoding
  also generalize better to non-standard dialects without explicit training. We suggest
  metrics to evaluate tokenizers and extensive model-independent analyses demonstrate
  the efficacy of the encoding when used together with data-driven subword tokenizers.

  '
authors: Samson Tan, Shafiq Joty, Lav R., and Min-Yen Kan
bibtex: "@article{tan-et-al-arxiv-20,\n abstract = {Morphological inflection is a\
  \ process of word formation where base words are modified to express different grammatical\
  \ categories such as tense, case, voice, person, or number. World Englishes, such\
  \ as Colloquial Singapore English (CSE) and African American Vernacular English\
  \ (AAVE), differ from Standard English dialects in inflection use. Although comprehension\
  \ by human readers is usually unimpaired by non-standard inflection use, NLP systems\
  \ are not so robust. We introduce a new Base-Inflection Encoding of English text\
  \ that is achieved by combining linguistic and statistical techniques. Fine-tuning\
  \ pre-trained NLP models for downstream tasks under this novel encoding achieves\
  \ robustness to non-standard inflection use while maintaining performance on Standard\
  \ English examples. Models using this encoding also generalize better to non-standard\
  \ dialects without explicit training. We suggest metrics to evaluate tokenizers\
  \ and extensive model-independent analyses demonstrate the efficacy of the encoding\
  \ when used together with data-driven subword tokenizers.},\n author = {Samson Tan\
  \ and Shafiq Joty and Lav R. Varshney and Min-Yen Kan},\n issue = {},\n journal\
  \ = {arXiv (* not peer reviewed)},\n link = {https://arxiv.org/abs/2004.14870},\n\
  \ pages = {},\n publisher = {arXiv.org},\n title = {Mind Your Inflections! Improving\
  \ NLP for Non-Standard English with Base-Inflection Encoding},\n year = {2020}\n\
  }\n"
code: null
doc-url: https://arxiv.org/abs/2004.14870
errata: null
id: tan-et-al-arxiv-20
img: tan-et-al-arxiv-20-fig
journal: arXiv (* not peer reviewed)
layout: singlepaper
pages: null
paper-type: article
picture: shafiq
selected: true
slides: null
title: 'Mind Your Inflections! Improving NLP for Non-Standard English with Base-Inflection
  Encoding

  '
venue: journal
year: 2020
---

{% include singlepaper.html paper=page %}