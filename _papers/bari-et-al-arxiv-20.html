---
abstract: 'Transfer learning has yielded state-of-the-art results in many supervised
  natural language processing tasks. However, annotated data for every target task
  in every target language is rare, especially for low-resource languages. In this
  work, we propose MultiMix, a novel data augmentation method for semi-supervised
  learning in zero-shot transfer learning scenarios. In particular, MultiMix targets
  to solve cross-lingual adaptation problems from a source (language) distribution
  to an unknown target (language) distribution assuming it has no training labels
  in the target language task. In its heart, MultiMix performs simultaneous self-training
  with data augmentation and unsupervised sample selection. To show its effectiveness,
  we have performed extensive experiments on zero-shot transfers for cross-lingual
  named entity recognition (XNER) and natural language inference (XNLI). Our experiments
  show sizeable improvements in both tasks outperforming the baselines by a good margin.

  '
authors: M Saiful, Tasnim Mohiuddin, and Shafiq Joty
bibtex: "@article{bari-et-al-arxiv-20,\n abstract = {Transfer learning has yielded\
  \ state-of-the-art results in many supervised natural language processing tasks.\
  \ However, annotated data for every target task in every target language is rare,\
  \ especially for low-resource languages. In this work, we propose MultiMix, a novel\
  \ data augmentation method for semi-supervised learning in zero-shot transfer learning\
  \ scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems\
  \ from a source (language) distribution to an unknown target (language) distribution\
  \ assuming it has no training labels in the target language task. In its heart,\
  \ MultiMix performs simultaneous self-training with data augmentation and unsupervised\
  \ sample selection. To show its effectiveness, we have performed extensive experiments\
  \ on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural\
  \ language inference (XNLI). Our experiments show sizeable improvements in both\
  \ tasks outperforming the baselines by a good margin.},\n author = {M Saiful Bari\
  \ and Tasnim Mohiuddin and Shafiq Joty},\n issue = {},\n journal = {arXiv (* not\
  \ peer reviewed)},\n link = {https://arxiv.org/abs/2004.13889},\n pages = {},\n\
  \ publisher = {arXiv.org},\n title = {MultiMix: A Robust Data Augmentation Strategy\
  \ for Cross-Lingual NLP},\n year = {2020}\n}\n"
code: null
doc-url: https://arxiv.org/abs/2004.13889
errata: null
id: bari-et-al-arxiv-20
img: bari-et-al-arxiv-20-fig
journal: arXiv (* not peer reviewed)
layout: singlepaper
pages: null
paper-type: article
picture: shafiq
selected: true
slides: null
title: 'MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP

  '
venue: journal
year: 2020
---

{% include singlepaper.html paper=page %}