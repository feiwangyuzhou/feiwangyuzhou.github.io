---
abstract: 'Although coherence modeling has come a long way in developing novel models,
  their evaluation on downstream applications has largely been neglected. With the
  advancements made by neural approaches in applications such as machine translation,
  text summarization and dialogue systems, the need for standard coherence evaluation
  is now more crucial than ever. In this paper, we propose to benchmark coherence
  models on a number of synthetic and downstream tasks. In particular, we evaluate
  well-known traditional and neural coherence models on sentence ordering tasks, and
  also on three downstream applications including coherence evaluation for machine
  translation, summarization and next utterance prediction. We also show model produced
  rankings for pre-trained language model outputs as another use-case. Our results
  demonstrate a weak correlation between the model performances in the synthetic tasks
  and the downstream applications, motivating alternate evaluation methods for coherence
  models. This work has led us to create a leaderboard to foster further research
  in coherence modeling.

  '
authors: Tasnim Mohiuddin, Prathyusha Jwalapuram, Xiang Lin, and Shafiq Joty
bibtex: "@article{bari-et-al-arxiv-20,\n abstract = {Although coherence modeling has\
  \ come a long way in developing novel models, their evaluation on downstream applications\
  \ has largely been neglected. With the advancements made by neural approaches in\
  \ applications such as machine translation, text summarization and dialogue systems,\
  \ the need for standard coherence evaluation is now more crucial than ever. In this\
  \ paper, we propose to benchmark coherence models on a number of synthetic and downstream\
  \ tasks. In particular, we evaluate well-known traditional and neural coherence\
  \ models on sentence ordering tasks, and also on three downstream applications including\
  \ coherence evaluation for machine translation, summarization and next utterance\
  \ prediction. We also show model produced rankings for pre-trained language model\
  \ outputs as another use-case. Our results demonstrate a weak correlation between\
  \ the model performances in the synthetic tasks and the downstream applications,\
  \ motivating alternate evaluation methods for coherence models. This work has led\
  \ us to create a leaderboard to foster further research in coherence modeling.},\n\
  \ author = {Tasnim Mohiuddin and Prathyusha Jwalapuram and Xiang Lin and Shafiq\
  \ Joty},\n issue = {},\n journal = {arXiv (* not peer reviewed)},\n link = {https://arxiv.org/abs/2004.13889},\n\
  \ pages = {},\n publisher = {arXiv.org},\n title = {CohEval: Benchmarking Coherence\
  \ Models},\n year = {2020}\n}\n"
code: null
doc-url: https://arxiv.org/abs/2004.13889
errata: null
id: bari-et-al-arxiv-20
img: bari-et-al-arxiv-20-fig
journal: arXiv (* not peer reviewed)
layout: singlepaper
pages: null
paper-type: article
picture: shafiq
selected: true
slides: null
title: 'CohEval: Benchmarking Coherence Models

  '
venue: journal
year: 2020
---

{% include singlepaper.html paper=page %}