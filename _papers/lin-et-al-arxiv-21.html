---
abstract: 'The success of Neural Machine Translation (NMT) largely depends on the
  availability of large bitext training corpora. Due to the lack of such large corpora
  in low-resource language pairs,  NMT systems often exhibit poor performance. Extra
  relevant monolingual data often helps, but acquiring it could be quite expensive,
  especially for low-resource languages. Moreover, domain mismatch between bitext
  (train/test) and monolingual data might degrade the performance. To alleviate such
  issues, we propose AugVic, a novel data augmentation framework for low-resource
  NMT which exploits the vicinal samples of the given bitext without using any extra
  monolingual data explicitly. It can diversify the in-domain bitext data with finer
  level control. Through extensive experiments on four low-resource language pairs
  comprising data from different domains, we have shown that our method is comparable
  to the traditional back-translation that uses extra in-domain monolingual data.
  When we combine the synthetic parallel data generated from AugVic with the ones
  from the extra monolingual data, we achieve further improvements. We show that AugVic
  helps to attenuate the discrepancies between relevant and distant-domain monolingual
  data in traditional back-translation. To understand the contributions of different
  components of AugVic, we perform an in-depth framework analysis.

  '
authors: Xiang Lin, Simeng Han, and Shafiq Joty
bibtex: "@inproceedings{lin-et-al-arxiv-21,\n abstract = {The success of Neural Machine\
  \ Translation (NMT) largely depends on the availability of large bitext training\
  \ corpora. Due to the lack of such large corpora in low-resource language pairs,\
  \  NMT systems often exhibit poor performance. Extra relevant monolingual data often\
  \ helps, but acquiring it could be quite expensive, especially for low-resource\
  \ languages. Moreover, domain mismatch between bitext (train/test) and monolingual\
  \ data might degrade the performance. To alleviate such issues, we propose AugVic,\
  \ a novel data augmentation framework for low-resource NMT which exploits the vicinal\
  \ samples of the given bitext without using any extra monolingual data explicitly.\
  \ It can diversify the in-domain bitext data with finer level control. Through extensive\
  \ experiments on four low-resource language pairs comprising data from different\
  \ domains, we have shown that our method is comparable to the traditional back-translation\
  \ that uses extra in-domain monolingual data. When we combine the synthetic parallel\
  \ data generated from AugVic with the ones from the extra monolingual data, we achieve\
  \ further improvements. We show that AugVic helps to attenuate the discrepancies\
  \ between relevant and distant-domain monolingual data in traditional back-translation.\
  \ To understand the contributions of different components of AugVic, we perform\
  \ an in-depth framework analysis.},\n address = {Virtual},\n author = {Xiang Lin\
  \ and Simeng Han and Shafiq Joty},\n booktitle = {In Thirty-eighth International\
  \ Conference on Machine Learning},\n link = {https://openreview.net/pdf?id=JAlqRs9duhz},\n\
  \ numpages = {9},\n publisher = {},\n series = {ICML'21 (long 3%)},\n title = {Straight\
  \ to the Gradient: Learning to Use Novel Tokens for Neural Text Generation},\n year\
  \ = {2021}\n}\n"
booktitle: 'In Thirty-eighth International Conference on Machine Learning (<b>ICML''21
  (long 3%)</b>)

  '
code: null
doc-url: https://openreview.net/pdf?id=JAlqRs9duhz
errata: null
id: lin-et-al-arxiv-21
img: lin-et-al-arxiv-21-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/lin-et-al-arxiv-21-slides.pdf
title: 'Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}