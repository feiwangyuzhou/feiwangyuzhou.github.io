---
abstract: 'Participants in an asynchronous conversation (e.g., forums, emails) interact
  with each other at different times, performing certain communicative acts, called
  speech acts (e.g., question, request). In this article, we propose a hybrid approach
  to speech act recognition in asynchronous conversations. Our approach works in two
  steps: a long short-term memory (LSTM) recurrent neural network first encodes each
  sentence separately into a distribution representation, which are then used in a
  conditional structured model to capture the conversational dependencies between
  sentences. The structured model can consider arbitrary graph structures to model
  conversational dependencies in an asynchronous conversation. In addition, to mitigate
  the problem of limited annotated data in the asynchronous domains, we adapt the
  recurrent model to learn from synchronous conversations (e.g., meetings) using adversarial
  training of neural networks. Empirical evaluation shows the effectiveness of our
  approach over existing ones: (i) LSTMs provide better task-specific representations,
  (ii) the global structured model improves over local models, and (iii) adversarial
  training gives better domain-invariant representations.

  '
authors: Shafiq Joty, and Shafiq Joty
bibtex: "@article{joty-cl-si-18,\n abstract = {Participants in an asynchronous conversation\
  \ (e.g., forums, emails) interact with each other\nat different times, performing\
  \ certain communicative acts, called speech acts (e.g., question, request). In this\
  \ article, we propose a hybrid approach to speech act recognition in asynchronous\
  \ conversations. Our approach works in two steps: a long short-term memory (LSTM)\
  \ recurrent neural network first encodes each sentence separately into a distribution\
  \ representation, which are then used in a conditional structured model to capture\
  \ the conversational dependencies between sentences. The structured model can consider\
  \ arbitrary graph structures to model conversational dependencies in an asynchronous\
  \ conversation. In addition, to mitigate the problem of limited annotated data in\
  \ the asynchronous domains, we adapt the recurrent model to learn from synchronous\
  \ conversations (e.g., meetings) using adversarial training of neural networks.\
  \ Empirical evaluation shows the effectiveness of our approach over existing ones:\
  \ (i) LSTMs provide better task-specific representations, (ii) the global structured\
  \ model improves over local models, and (iii) adversarial training gives better\
  \ domain-invariant representations.},\n author = {Shafiq Joty and Shafiq Joty},\n\
  \ issue = {(Under review)},\n journal = {Computational Linguistics (Special Issue\
  \ on Language in Social Media, Exploiting discourse and other contextual information)},\n\
  \ link = {},\n pages = {},\n publisher = {MIT Press},\n title = {Speech Act Modeling\
  \ of Written Asynchronous Conversations: A Neural CRF Approach},\n year = {2018}\n\
  }\n"
code: null
doc-url: null
errata: null
id: joty-cl-si-18
img: joty-cl-si-18-fig
journal: Computational Linguistics (Special Issue on Language in Social Media, Exploiting
  discourse and other contextual information)
layout: singlepaper
pages: null
paper-type: article
picture: shafiq
selected: true
slides: null
title: 'Speech Act Modeling of Written Asynchronous Conversations: A Neural CRF Approach

  '
venue: journal
year: 2018
---

{% include singlepaper.html paper=page %}