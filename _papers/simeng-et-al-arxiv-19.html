---
abstract: 'Submodularity is a desirable property for a variety of objectives in summarization
  in terms of content selection where the current encode-decoder framework is deficient.
  We propose diminishing attentions, a class of novel attention mechanisms. They are
  architecturally simple yet empirically effective to improve the coverage of neural
  abstractive summarization by exploiting the properties of submodular functions.
  Without adding any extra parameters to the Pointer-Generator baseline, our attention
  mechanism yields significant improvements in ROUGE scores and generates summaries
  of better quality measured from several aspects. Our method within the Pointer-Generator
  framework outperforms the recently proposed Transformer-based model for summarization
  while using only 5 times less parameters. When applied to the encoder-decoder attention
  in the Transformer model initialized with BERT, our method also achieves state-of-the-art
  results in abstractive summarization.

  '
authors: Simeng Han, Xiang Lin, and Shafiq Joty
bibtex: "@article{simeng-et-al-arxiv-19,\n abstract = {Submodularity is a desirable\
  \ property for a variety of objectives in summarization in terms of content selection\
  \ where the current encode-decoder framework is deficient. We propose diminishing\
  \ attentions, a class of novel attention mechanisms. They are architecturally simple\
  \ yet empirically effective to improve the coverage of neural abstractive summarization\
  \ by exploiting the properties of submodular functions. Without adding any extra\
  \ parameters to the Pointer-Generator baseline, our attention mechanism yields significant\
  \ improvements in ROUGE scores and generates summaries of better quality measured\
  \ from several aspects. Our method within the Pointer-Generator framework outperforms\
  \ the recently proposed Transformer-based model for summarization while using only\
  \ 5 times less parameters. When applied to the encoder-decoder attention in the\
  \ Transformer model initialized with BERT, our method also achieves state-of-the-art\
  \ results in abstractive summarization.},\n author = {Simeng Han and Xiang Lin and\
  \ Shafiq Joty},\n issue = {},\n journal = {arXiv (not peer reviewed)},\n link =\
  \ {https://arxiv.org/abs/1911.03014},\n pages = {},\n publisher = {arXiv.org},\n\
  \ title = {Resurrecting Submodularity in Neural Abstractive Summarization},\n year\
  \ = {2019}\n}\n"
code: null
doc-url: https://arxiv.org/abs/1911.03014
errata: null
id: simeng-et-al-arxiv-19
img: simeng-et-al-arxiv-19-fig
journal: arXiv (not peer reviewed)
layout: singlepaper
pages: null
paper-type: article
picture: shafiq
selected: true
slides: null
title: 'Resurrecting Submodularity in Neural Abstractive Summarization

  '
venue: journal
year: 2019
---

{% include singlepaper.html paper=page %}