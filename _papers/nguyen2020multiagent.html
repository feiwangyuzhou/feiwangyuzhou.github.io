---
abstract: 'Recent unsupervised machine translation (UMT) systems usually employ three
  main principles: initialization, language modeling and iterative back-translation,
  though they may apply these principles differently. This work introduces another
  component to this framework: Multi-Agent Cross-translated Diversification (MACD).
  The method trains multiple UMT agents and then translates monolingual data back
  and forth using non-duplicative agents to acquire synthetic parallel data for supervised
  MT. MACD is applicable to all previous UMT approaches. In our experiments, the technique
  boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular,
  in WMT''14 English-French, WMT''16 German-English and English-Romanian, MACD outperforms
  cross-lingual masked language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively.
  It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German
  translation tasks. Through extensive experimental analyses, we show that MACD is
  effective because it embraces data diversity while other similar variants do not.

  '
authors: Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, and Ai Ti
bibtex: "@misc{nguyen2020multiagent,\n abstract = {Recent unsupervised machine translation\
  \ (UMT) systems usually employ three main principles: initialization, language modeling\
  \ and iterative back-translation, though they may apply these principles differently.\
  \ This work introduces another component to this framework: Multi-Agent Cross-translated\
  \ Diversification (MACD). The method trains multiple UMT agents and then translates\
  \ monolingual data back and forth using non-duplicative agents to acquire synthetic\
  \ parallel data for supervised MT. MACD is applicable to all previous UMT approaches.\
  \ In our experiments, the technique boosts the performance for some commonly used\
  \ UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French, WMT'16 German-English\
  \ and English-Romanian, MACD outperforms cross-lingual masked language model pretraining\
  \ by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5-3.3 BLEU improvements\
  \ in IWSLT English-French and English-German translation tasks. Through extensive\
  \ experimental analyses, we show that MACD is effective because it embraces data\
  \ diversity while other similar variants do not.},\n archiveprefix = {arXiv},\n\
  \ author = {Xuan-Phi Nguyen and Shafiq Joty and Wu Kui and Ai Ti Aw},\n eprint =\
  \ {2006.02163},\n link = {https://arxiv.org/abs/2006.02163},\n title = {Multi-Agent\
  \ Cross-Translated Diversification for Unsupervised Machine Translation},\n year\
  \ = {2020}\n}\n"
code: null
doc-url: https://arxiv.org/abs/2006.02163
errata: null
id: nguyen2020multiagent
img: nguyen2020multiagent-fig
layout: singlepaper
paper-type: misc
picture: shafiq
selected: false
slides: media/nguyen2020multiagent-slides.pdf
title: 'Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation

  '
venue: conference
year: 2020
---

{% include singlepaper.html paper=page %}