---
abstract: 'Recent unsupervised machine translation (UMT) systems usually employ three
  main principles: initialization, language modeling and iterative back-translation,
  though they may apply these principles differently. This work introduces another
  component to this framework: Multi-Agent Cross-translated Diversification (MACD).
  The method trains multiple UMT agents and then translates monolingual data back
  and forth using non-duplicative agents to acquire synthetic parallel data for supervised
  MT. MACD is applicable to all previous UMT approaches. In our experiments, the technique
  boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular,
  in WMT''14 English-French, WMT''16 German-English and English-Romanian, MACD outperforms
  cross-lingual masked language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively.
  It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German
  translation tasks. Through extensive experimental analyses, we show that MACD is
  effective because it embraces data diversity while other similar variants do not.

  '
authors: Xuan-Phi Nguyen, Shafiq Joty, Thanh-Tung Nguyen, Wu Kui, and Ai Ti
bibtex: "@inproceedings{nguyen2020multiagent,\n abstract = {Recent unsupervised machine\
  \ translation (UMT) systems usually employ three main principles: initialization,\
  \ language modeling and iterative back-translation, though they may apply these\
  \ principles differently. This work introduces another component to this framework:\
  \ Multi-Agent Cross-translated Diversification (MACD). The method trains multiple\
  \ UMT agents and then translates monolingual data back and forth using non-duplicative\
  \ agents to acquire synthetic parallel data for supervised MT. MACD is applicable\
  \ to all previous UMT approaches. In our experiments, the technique boosts the performance\
  \ for some commonly used UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French,\
  \ WMT'16 German-English and English-Romanian, MACD outperforms cross-lingual masked\
  \ language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields\
  \ 1.5-3.3 BLEU improvements in IWSLT English-French and English-German translation\
  \ tasks. Through extensive experimental analyses, we show that MACD is effective\
  \ because it embraces data diversity while other similar variants do not.},\n address\
  \ = {Virtual},\n author = {Xuan-Phi Nguyen and Shafiq Joty and Thanh-Tung Nguyen\
  \ and Wu Kui and Ai Ti Aw},\n booktitle = {In Thirty-eighth International Conference\
  \ on Machine Learning},\n link = {https://arxiv.org/abs/2006.02163},\n numpages\
  \ = {9},\n publisher = {},\n series = {ICML'21},\n title = {Cross-model Back-translated\
  \ Distillation for Unsupervised Machine Translation},\n year = {2021}\n}\n"
booktitle: 'In Thirty-eighth International Conference on Machine Learning (<b>ICML''21</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2006.02163
errata: null
id: nguyen2020multiagent
img: nguyen2020multiagent-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/nguyen2020multiagent-slides.pdf
title: 'Cross-model Back-translated Distillation for Unsupervised Machine Translation

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}