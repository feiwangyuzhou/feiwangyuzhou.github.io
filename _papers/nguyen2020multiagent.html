---
abstract: 'Recent unsupervised machine translation (UMT) systems usually employ three
  main principles: initialization, language modeling and iterative back-translation,
  though they may apply these principles differently. This work introduces another
  component to this framework: Multi-Agent Cross-translated Diversification (MACD).
  The method trains multiple UMT agents and then translates monolingual data back
  and forth using non-duplicative agents to acquire synthetic parallel data for supervised
  MT. MACD is applicable to all previous UMT approaches. In our experiments, the technique
  boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular,
  in WMT''14 English-French, WMT''16 German-English and English-Romanian, MACD outperforms
  cross-lingual masked language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively.
  It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German
  translation tasks. Through extensive experimental analyses, we show that MACD is
  effective because it embraces data diversity while other similar variants do not.

  '
authors: Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, and Ai Ti
bibtex: "@article{nguyen2020multiagent,\n abstract = {Recent unsupervised machine\
  \ translation (UMT) systems usually employ three main principles: initialization,\
  \ language modeling and iterative back-translation, though they may apply these\
  \ principles differently. This work introduces another component to this framework:\
  \ Multi-Agent Cross-translated Diversification (MACD). The method trains multiple\
  \ UMT agents and then translates monolingual data back and forth using non-duplicative\
  \ agents to acquire synthetic parallel data for supervised MT. MACD is applicable\
  \ to all previous UMT approaches. In our experiments, the technique boosts the performance\
  \ for some commonly used UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French,\
  \ WMT'16 German-English and English-Romanian, MACD outperforms cross-lingual masked\
  \ language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields\
  \ 1.5-3.3 BLEU improvements in IWSLT English-French and English-German translation\
  \ tasks. Through extensive experimental analyses, we show that MACD is effective\
  \ because it embraces data diversity while other similar variants do not.},\n author\
  \ = {Xuan-Phi Nguyen and Shafiq Joty and Wu Kui and Ai Ti Aw},\n issue = {},\n journal\
  \ = {arXiv (* not peer reviewed)},\n link = {https://arxiv.org/abs/2006.02163},\n\
  \ pages = {},\n publisher = {arXiv.org},\n title = {Multi-Agent Cross-Translated\
  \ Diversification for Unsupervised Machine Translation},\n year = {2020}\n}\n"
code: null
doc-url: https://arxiv.org/abs/2006.02163
errata: null
id: nguyen2020multiagent
img: nguyen2020multiagent-fig
journal: arXiv (* not peer reviewed)
layout: singlepaper
pages: null
paper-type: article
picture: shafiq
selected: true
slides: null
title: 'Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation

  '
venue: journal
year: 2020
---

{% include singlepaper.html paper=page %}