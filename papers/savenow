@inproceedings{Nguyen-et-al-acl-21,
 author = {Thanh-Tung Nguyen and Xuan-Phi Nguyen and Shafiq Joty and Xiaoli Li},
 title = {A Conditional Splitting Framework for Efficient Constituency Parsing},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'21},
  year = {2021},
  address = {Bangkok, Thailand},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {We introduce a generic seq2seq parsing framework that casts constituency parsing problems (syntactic and discourse parsing) into a series of conditional splitting decisions. Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding, which is linear in number of nodes. The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without relying on expensive structured inference. Crucially, for discourse analysis we show that in our formulation, discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring  segmentation as a pre-requisite. Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours. In discourse parsing, our method outperforms SoTA by a good margin. Our source code will be publicly available.},
} 

@inproceedings{Mohiuddin-et-al-acl-21,
 author = {Tasnim Mohiuddin and M Saiful Bari and Shafiq Joty},
 title = {AugVic: Exploiting BiText Vicinity for Low-Resource NMT},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'21},
  year = {2021},
  address = {Bangkok, Thailand},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {The success of Neural Machine Translation (NMT) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs,  NMT systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train/test) and monolingual data might degrade the performance. To alleviate such issues, we propose AugVic, a novel data augmentation framework for low-resource NMT which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with finer level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from AugVic with the ones from the extra monolingual data, we achieve further improvements. We show that AugVic helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of AugVic, we perform an in-depth framework analysis.},
} 

@inproceedings{lin-et-al-arxiv-21,
   title="{Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation}",
   author={Xiang Lin and Simeng Han and Shafiq Joty},
   booktitle = {In Thirty-eighth International Conference on Machine Learning},
  series = {ICML'21},
  year = {2021},
  address = {Virtual},
  numpages = {9},
  publisher = {},
  url = {https://openreview.net/pdf?id=JAlqRs9duhz},
  abstract = {The success of Neural Machine Translation (NMT) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs,  NMT systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train/test) and monolingual data might degrade the performance. To alleviate such issues, we propose AugVic, a novel data augmentation framework for low-resource NMT which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with finer level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from AugVic with the ones from the extra monolingual data, we achieve further improvements. We show that AugVic helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of AugVic, we perform an in-depth framework analysis.},
} 


@inproceedings{Tan-et-al-acl-21,
 author = {Samson Tan and Shafiq Joty and Kathy Baxter and Araz Taeihagh and Gregory A. Bennett and Min-Yen Kan},
 title = {Reliability Testing for Natural Language Processing Systems: An Adversarial Perspective},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'21},
  year = {2021},
  address = {Bangkok, Thailand},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {https://openreview.net/pdf?id=7ZL84tVlHZN}, 
  abstract = {Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly \emph{and} function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing --- with an emphasis on interdisciplinary collaboration --- will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.},
} 

@inproceedings{linlin-et-al-acl-21,
 author = {Linlin Liu and Bosheng Ding and Lidong Bing and Shafiq Joty and Luo Si and Chunyan Miao},
 title = {MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'21},
  year = {2021},
  address = {Bangkok, Thailand},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages. These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages. The code and data in this work will be made public for the research community.},
} 

@inproceedings{bari-et-al-arxiv-20,
  title="{UXLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual NLP}",
  author={M Saiful Bari and Tasnim Mohiuddin and Shafiq Joty},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'21 (under review)},
  year = {2021},
  address = {Bangkok, Thailand},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  url = {https://arxiv.org/abs/2004.13889},
  abstract={Transfer learning has yielded state-of-the-art results in many supervised natural language processing tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. In this work, we propose MultiMix, a novel data augmentation method for semi-supervised learning in zero-shot transfer learning scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems from a source (language) distribution to an unknown target (language) distribution assuming it has no training labels in the target language task. In its heart, MultiMix performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we have performed extensive experiments on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural language inference (XNLI). Our experiments show sizeable improvements in both tasks outperforming the baselines by a good margin.},
}

