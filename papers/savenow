

@InProceedings{Yue-emnlp-21,
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven Hoi},
  title     = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model is unified in that it builds on a unified framework to seamlessly support both code understanding and generation tasks, and it employs a unified format of task control codes to allow for multi-task learning. We propose a novel identifier-aware pre-training objective that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. To further close the gap between the pre-training and fine-tuning, we propose a bimodal dual generation task to encourage the alignment between NL and PL. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. 
Further analysis reveals that our model can better capture semantic information from code.}
}


@InProceedings{Tao-emnlp-21,
  author = {Tao Yu and Shafiq Joty},
  title     = {Effective Fine-tuning Methods for Cross-lingual Adaptation},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as complementary to multilingual language modeling using the unlabeled data in the target language.  We also propose an adaption method based on contrastive learning to better capture the semantic relationship in the parallel data, when a few translation pairs are available. To show our method's effectiveness, we conduct extensive experiments on cross-lingual inference and review classification tasks across various languages. We report significant gains compared to directly fine-tuning multilingual pre-trained models and other semi-supervised alternatives.\footnote{Code and models are available at \scriptsize{\urlstyle{tt}\url{<redacted>}}}.}
}



@InProceedings{Yingzhu-et-al-emnlp-21,
  author    = {Yingzhu Zhao and Chongjia Ni and Cheung-Chi LEUNG and Shafiq Joty and Eng Siong Chng and Bin Ma},
  title     = {A Unified Speaker Adaptation Approach for ASR},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52\% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.1\% relative WER reduction, and surpasses the finetuning method by up to relative 8.62\%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53\% with only a few epochs of training.}
}

@article{Saha-aaai-2022,
  title="{Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning}",
  author={Amrita Saha, Shafiq Joty, Steven C.H. Hoi},
  journal = {arXiv (* not peer reviewed)},
  publisher={arXiv.org},
  issue={},
  pages={},
  year={2022},
  url = {https://arxiv.org/abs/2101.11802},
  abstract={Neural Module Networks (NMNs) have been quite successful in incorporating explicit reasoning as learnable modules in various question answering tasks, including
the most generic form of numerical reasoning over text in Machine Reading Comprehension (MRC). However, to achieve this, contemporary NMNs need strong
supervision in executing the query as a specialized program over reasoning modules
and fail to generalize to more open-ended settings without such supervision. Hence
we propose Weakly-Supervised Neuro-Symbolic Module Network (WNSMN)
trained with answers as the sole supervision for numerical reasoning based MRC. It
learns to execute a noisy heuristic program obtained from the dependency parsing
of the query, as discrete actions over both neural and symbolic reasoning modules
and trains it end-to-end in a reinforcement learning framework with discrete reward
from answer matching. On the numerical-answer subset of DROP, WNSMN outperforms NMN by 32% and the reasoning-free language model GenBERT by 8%
in exact match accuracy when trained under comparable weak supervised settings.
This showcases the effectiveness and generalizability of modular networks that can
handle explicit discrete reasoning over noisy programs in an end-to-end manner.
}
}




SAL33


pfnjld




2408.45, 2500
2312.90, 0ut 2221.25

2617.22, 2500 = 127. 
2475.73, out 2603.10




Shafiq Joty is an Asst. Prof. in the School of Computer Science and Engineering (SCSE) at NTU, where he leads the NTU-NLP group. He is also a senior manager of NLP research and a founding member at Salesforce AI Research Asia. His work has primarily focused on developing language analysis tools (e.g., syntactic parsers, NER, discourse parser, coherence models) and downstream NLP applications including question answering, text summarization and language modeling. A significant part of his current research focuses on machine translation, multilingual processing, multimodal processing, and robustness of NLP models. His work has mostly relied on deep learning for better representation of the input text and on probabilistic graphical models and reinforcement learning for capturing dependencies in the output. He served as a (senior) area chair for ACL'19-21, EMNLP'19,21 and NAACL’21, EACL’21, and a senior PC member for AAAI’21 and IJCAI'19. He gave tutorials at ACL-2019, ICDM-2018 and COLING-2018 conferences. He is currently serving as an action editor for the ACL rolling review system (a centralized reviewing system for ACL conferences), and previously served as an associate editor for ACM Transactions on Asian and Low Resource Language Processing. He has published more than 100 papers in top-tier NLP/AI conferences and journals including ACL, EMNLP, NAACL, NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, CL and JAIR. 






This paper extends the idea of prefix tuning to controllable generation, i.e., steer generation such that the resulting sequences carry a chosen attribute. On the positive side, the proposed method requires less number of parameters to be trained compared to other methods like GeDi. The results for the baselines reported in this paper seem to contradict with the origianl paper's findings (e.g., linguistic quality, degeneration) as identified by Reviewers rpJ2 and GHyw. The description of human evaluation needs to be improved with more details about the subjects, their agreements etc (Reviewers ZTVW and GHyw). Reviewer rpJ2 also pointed out two other weaknesses: the inability to counter the domain effect and an artificial unsupervised setup. Reviewer EjkZ and ZTVW question the novelty beyound prefix tuning and VQ-VAE although I personally think the unsupervised and the multi-attribute settings are quite interesting.   




1. Weishi Wang (NLP) - NTU - Advised by me + Steven (SF)
2. Hailin Chen (NLP) - NTU - Advised by me + Steven (SF)
3. Hualin Liu (Vision)  - NTU - Advised by Hanwang + Junnan (SF)
4. Anthony Meng Huat (Vision) - NTU - started with Lin Guosheng, then transferred to Boyang "Albert" Li + Junnan (SF)
5. Samson Tag (NLP) - NUS - Advised by Min-Yen + me (SF)
6. Gerald Woo (AIOps) - SMU - 




