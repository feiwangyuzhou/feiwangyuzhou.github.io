

@InProceedings{Yue-emnlp-21,
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven Hoi},
  title     = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model is unified in that it builds on a unified framework to seamlessly support both code understanding and generation tasks, and it employs a unified format of task control codes to allow for multi-task learning. We propose a novel identifier-aware pre-training objective that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. To further close the gap between the pre-training and fine-tuning, we propose a bimodal dual generation task to encourage the alignment between NL and PL. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. 
Further analysis reveals that our model can better capture semantic information from code.}
}


@InProceedings{Tao-emnlp-21,
  author = {Tao Yu and Shafiq Joty},
  title     = {Effective Fine-tuning Methods for Cross-lingual Adaptation},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as complementary to multilingual language modeling using the unlabeled data in the target language.  We also propose an adaption method based on contrastive learning to better capture the semantic relationship in the parallel data, when a few translation pairs are available. To show our method's effectiveness, we conduct extensive experiments on cross-lingual inference and review classification tasks across various languages. We report significant gains compared to directly fine-tuning multilingual pre-trained models and other semi-supervised alternatives.\footnote{Code and models are available at \scriptsize{\urlstyle{tt}\url{<redacted>}}}.}
}



@InProceedings{Yingzhu-et-al-emnlp-21,
  author    = {Yingzhu Zhao and Chongjia Ni and Cheung-Chi LEUNG and Shafiq Joty and Eng Siong Chng and Bin Ma},
  title     = {A Unified Speaker Adaptation Approach for ASR},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52\% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.1\% relative WER reduction, and surpasses the finetuning method by up to relative 8.62\%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53\% with only a few epochs of training.}
}

@inproceedings{Junnan-et-al-nips-21,
  title="{Align before Fuse: Vision and Language Representation Learning with
               Momentum Distillation}",
  author    = {Junnan Li and
               Ramprasaath R. Selvaraju and
               Akhilesh Deepak Gotmare and
               Shafiq Joty and
               Caiming Xiong and
               Steven C. H. Hoi},
  booktitle = {2021 Conference on Neural Information Processing Systems},
  address = {Online},
  series = {NeurIPS'21 (under review)},
  year={2021},
  url = {https://arxiv.org/abs/2107.07651},
  abstract={Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR2, ALBEF achieves absolute improvements of 2.37\% and 3.84\% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at .}
}





SAL33


pfnjld




2408.45, 2500
2312.90, 0ut 2221.25

2617.22, 2500 = 127. 
2475.73, out 2603.10




Shafiq Joty is an Asst. Prof. in the School of Computer Science and Engineering (SCSE) at NTU, where he leads the NTU-NLP group. He is also a senior manager of NLP research and a founding member at Salesforce AI Research Asia. His work has primarily focused on developing language analysis tools (e.g., syntactic parsers, NER, discourse parser, coherence models) and downstream NLP applications including question answering, text summarization and language modeling. A significant part of his current research focuses on machine translation, multilingual processing, multimodal processing, and robustness of NLP models. His work has mostly relied on deep learning for better representation of the input text and on probabilistic graphical models and reinforcement learning for capturing dependencies in the output. He served as a (senior) area chair for ACL'19-21, EMNLP'19,21 and NAACL’21, EACL’21, and a senior PC member for AAAI’21 and IJCAI'19. He gave tutorials at ACL-2019, ICDM-2018 and COLING-2018 conferences. He is currently serving as an action editor for the ACL rolling review system (a centralized reviewing system for ACL conferences), and previously served as an associate editor for ACM Transactions on Asian and Low Resource Language Processing. He has published more than 100 papers in top-tier NLP/AI conferences and journals including ACL, EMNLP, NAACL, NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, CL and JAIR. 






This paper extends the idea of prefix tuning to controllable generation, i.e., steer generation such that the resulting sequences carry a chosen attribute. On the positive side, the proposed method requires less number of parameters to be trained compared to other methods like GeDi. The results for the baselines reported in this paper seem to contradict with the origianl paper's findings (e.g., linguistic quality, degeneration) as identified by Reviewers rpJ2 and GHyw. 

Reviewer rpJ2 also pointed out two other weaknesses: the inability to counter the domain effect and an artificial unsupervised setup. Reviewer EjkZ also questions the novelty beyound prefix tuning and VQ-VAE although I personally think the unsupervised and the multi-attribute settings are quite interesting.   






